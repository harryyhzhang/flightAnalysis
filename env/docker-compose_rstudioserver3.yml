version: "3"

services:

  namenode:
    image: harryyhzhang/hadoopeco
    ports:
      - "50070:50070"
      - "8090:8090"
      - "8088:8088"
    hostname: test-master
    container_name: test-master
    entrypoint:  sh -c  "sudo service ssh start &&  
                    echo 'test-slave-1\ntest-slave-2' > /usr/local/hadoop/etc/hadoop/slaves  &&
                    /usr/local/hadoop/sbin/start-dfs.sh &&  
                    /usr/local/hadoop/sbin/start-yarn.sh &&  
                    /usr/local/spark/sbin/start-all.sh &&  
                    hadoop fs -mkdir -p /user/rstudio-user &&  
                    hadoop fs -chmod 777 /user/rstudio-user && 
                    sudo /usr/lib/rstudio-server/bin/rstudio-server start && 
                    bash" 
    volumes:
      - test-master-volume:/hadoop/dfs/name
      - ../data:/data
    tty: true
    stdin_open: true
    networks:
      webnet:
        ipv4_address: 192.168.33.2
    deploy:
      resources:
          limits:
            memory: 4g
            
  datanode1:
    image: harryyhzhang/hadoopeco
    ports: 
      - "8042:8042"
    volumes:
      - test-slave-1-volume:/hadoop/dfs/data
    hostname: test-slave-1
    container_name: test-slave-1
    tty: true
    stdin_open: true
    networks:
      webnet:
        ipv4_address: 192.168.33.3
    deploy:
      resources:
          limits:
            memory: 4g

  datanode2:
    image: harryyhzhang/hadoopeco
    ports: 
      - "8043:8042"
    volumes:
      - test-slave-2-volume:/hadoop/dfs/data
    hostname: test-slave-2
    container_name: test-slave-2
    tty: true
    stdin_open: true
    networks:
      webnet:
        ipv4_address: 192.168.33.4
    deploy:
      resources:
          limits:
            memory: 4g


networks:
 webnet:
   external:
     name: shared_nw

volumes:
   test-master-volume: 
   test-slave-1-volume:
   test-slave-2-volume:
   test-slave-3-volume:
   
